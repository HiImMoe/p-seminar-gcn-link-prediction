{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a45fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import FullBatchLinkGenerator\n",
    "from stellargraph.layer import GCN, LinkEmbedding\n",
    "from stellargraph import datasets\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "from stellargraph import globalvar\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_general_metrics(byte_array, row_data):\n",
    "    unigram_dist_int_chars = [0] * 256\n",
    "    hamming_ones = 0\n",
    "    hamming_total_bits = 0\n",
    "    mean_byte_sum_byte_values = 0\n",
    "    longest_streak = 0\n",
    "    for byte in byte_array:\n",
    "        ### unigram dist ###\n",
    "        unigram_dist_int_chars[int(byte.hex(), 16)] += 1\n",
    "        \n",
    "        ### hamming weight ###\n",
    "        bits = bin(int(byte.hex(), 16))[2:]\n",
    "        hamming_total_bits += len(bits)\n",
    "        for char in bits:\n",
    "            if char == '1':\n",
    "                hamming_ones += 1\n",
    "                \n",
    "        ### mean byte value ###\n",
    "        # convert byte to hex then into int\n",
    "        mean_byte_sum_byte_values += int(byte.hex(), 16)\n",
    "                \n",
    "        ### Longest Streak ###\n",
    "        i = byte_array.index(byte)\n",
    "        j = 0\n",
    "        temp_streak = 1\n",
    "        while i + j + 1 < len(byte_array) and byte_array[i + j] == byte_array[i + j + 1]:\n",
    "            temp_streak += 1\n",
    "            j += 1\n",
    "        if temp_streak > longest_streak:\n",
    "            longest_streak = temp_streak\n",
    "    \n",
    "    row_data[\"unigram_dist_mean\"], row_data[\"unigram_dist_std\"]  = norm.fit(unigram_dist_int_chars)\n",
    "    row_data[\"hamming_weight\"] = hamming_ones/hamming_total_bits\n",
    "    row_data[\"mean_byte_value\"] = mean_byte_sum_byte_values/len(byte_array)\n",
    "    row_data[\"longest_streak\"] = longest_streak\n",
    "    row_data[\"file_size\"] = len(byte_array)\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTables(cursor):\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5670dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeCols(cols, table, conn, row_data):\n",
    "    \n",
    "    for col in cols:\n",
    "        query = \"SELECT \" + col + \" FROM \" + table + ';'\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        if col != 'id' and df.dtypes[0] in [\"int64\", \"float64\"]:\n",
    "            row_data[\"num_of_int_cols\"] += 1\n",
    "            row_data[table + \"_\" + col + \"_mean\"] = df.mean()[0]\n",
    "            row_data[table + \"_\" + col + \"_std\"] = df.std()[0]\n",
    "        elif col != 'id':\n",
    "            r#ow_data[table + \"_\" + col + \"_avg_num_of_chars\"] = 0\n",
    "            row_data[\"num_of_text_cols\"] += 1\n",
    "            char_dist = [0] * 128\n",
    "            for row in df.index:\n",
    "                #row_data[table + \"_\" + col + \"_avg_num_of_chars\"] += len(df[col][row])\n",
    "                chars = 0\n",
    "                numeric = 0\n",
    "                for char in df[col][row]:\n",
    "                    if ord(char) <= 128:\n",
    "                        char_dist[ord(char)] += 1\n",
    "                    if char.isalpha():\n",
    "                        chars += 1\n",
    "                    elif char.isnumeric():\n",
    "                        numeric += 1\n",
    "            #row_data[table + \"_\" + col + \"_char_dist_mean\"] = norm.fit(char_dist)[0]\n",
    "            #row_data[table + \"_\" + col + \"_char_dist_std\"] = norm.fit(char_dist)[1]\n",
    "            #row_data[table + \"_\" + col + \"_avg_num_of_chars\"] = row_data[table + \"_\" + col + \"_avg_num_of_chars\"] / len(df.index)\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8456d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeTable(table, conn, row_data):\n",
    "    query = \"SELECT * FROM \" + table + ';'\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    row_data[\"num_of_cols\"] += len(df.columns)\n",
    "    row_data[\"num_of_rows\"] += len(df.index)\n",
    "    row_data[table + \"_rows\"] = len(df.index)\n",
    "    row_data[table + \"_cols\"] = len(df.columns)\n",
    "    row_data = analyzeCols(df.keys(), table, conn, row_data)\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b744e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeDB(db_path, db_name, row_data):\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    tables = getTables(cursor)\n",
    "    row_data[\"num_of_tables\"] = len(tables)\n",
    "\n",
    "    for table in tables:\n",
    "        row_data = analyzeTable(str(table)[2:-3], conn, row_data)\n",
    "    \n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn(sdg, train_size, test_size, epochs_input, layer_size_num):\n",
    "    \n",
    "    edge_splitter_test = EdgeSplitter(sdg)\n",
    "\n",
    "    G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "        p=test_size, method=\"global\", keep_connected=False\n",
    "    )\n",
    "    \n",
    "    edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "    G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "        p=train_size, method=\"global\", keep_connected=False\n",
    "    )\n",
    "    \n",
    "    epochs = epochs_input\n",
    "    \n",
    "    train_gen = FullBatchLinkGenerator(G_train, method=\"gcn\")\n",
    "    train_flow = train_gen.flow(edge_ids_train, edge_labels_train)\n",
    "    \n",
    "    test_gen = FullBatchLinkGenerator(G_test, method=\"gcn\")\n",
    "    test_flow = train_gen.flow(edge_ids_test, edge_labels_test)\n",
    "    \n",
    "    gcn = GCN(\n",
    "        layer_sizes=[layer_size_num, layer_size_num], activations=[\"relu\", \"relu\"], generator=train_gen, dropout=0.3\n",
    "    )\n",
    "    \n",
    "    x_inp, x_out = gcn.in_out_tensors()\n",
    "    \n",
    "    prediction = LinkEmbedding(activation=\"relu\", method=\"ip\")(x_out)\n",
    "    \n",
    "    prediction = keras.layers.Reshape((-1,))(prediction)\n",
    "    \n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    \n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "    \n",
    "    init_train_metrics = model.evaluate(train_flow)\n",
    "    init_test_metrics = model.evaluate(test_flow)\n",
    "\n",
    "    print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "    for name, val in zip(model.metrics_names, init_train_metrics):\n",
    "        print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "\n",
    "    print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
    "    for name, val in zip(model.metrics_names, init_test_metrics):\n",
    "        print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "        \n",
    "    history = model.fit(\n",
    "        train_flow, epochs=epochs, validation_data=test_flow, verbose=2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    sg.utils.plot_history(history)\n",
    "    \n",
    "    train_metrics = model.evaluate(train_flow)\n",
    "    test_metrics = model.evaluate(test_flow)\n",
    "\n",
    "    print(\"\\nTrain Set Metrics of the trained model:\")\n",
    "    for name, val in zip(model.metrics_names, train_metrics):\n",
    "        print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "        train_acc = val\n",
    "\n",
    "    print(\"\\nTest Set Metrics of the trained model:\")\n",
    "    for name, val in zip(model.metrics_names, test_metrics):\n",
    "        print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "    return train_metrics[1], train_metrics[0], test_metrics[1], test_metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_dbs = 100\n",
    "num_of_versions = 4\n",
    "data = []\n",
    "mapped_edges = []\n",
    "\n",
    "for db_num in range(num_of_dbs):\n",
    "    for version in range(num_of_versions):\n",
    "        byte_array = []\n",
    "        db_path = \"demo_dbs_2/db_\" + str(db_num) + \"_v\" + str(version) + \".sqlite\"\n",
    "        db_name = \"db_\" + str(db_num) + \"_v\" + str(version)\n",
    "        row_data = {\"db_name\": db_name,\n",
    "                    \"unigram_dist_mean\": 0,\n",
    "                    \"unigram_dist_std\": 0,\n",
    "                    \"hamming_weight\": 0,\n",
    "                    \"mean_byte_value\": 0,\n",
    "                    \"longest_streak\": 0,\n",
    "                    \"file_size\": 0,\n",
    "                    \"num_of_tables\": 0,\n",
    "                    \"num_of_cols\": 0,\n",
    "                    \"num_of_rows\": 0,\n",
    "                    \"num_of_text_cols\": 0,\n",
    "                    \"num_of_int_cols\": 0\n",
    "                   }\n",
    "\n",
    "        with open(\"demo_dbs_2/db_\" + str(db_num) + \"_v\" + str(version) + \".sqlite\", \"rb\") as f:\n",
    "            byte = f.read(1)\n",
    "            while byte:\n",
    "                byte_array.append(byte)\n",
    "                byte = f.read(1)\n",
    "        row_data = analyze_general_metrics(byte_array, row_data)\n",
    "        f.close()\n",
    "        \n",
    "        row_data = analyzeDB(db_path, db_name, row_data)\n",
    "        \n",
    "        data.append(row_data)\n",
    "        \n",
    "        if version < num_of_versions - 1:\n",
    "            db_name_next = \"db_\" + str(db_num) + \"_v\" + str(version + 1)\n",
    "            mapped_edges.append([db_name, db_name_next])\n",
    "        clear_output(wait=True)\n",
    "        print(\"Progress: \" + str(((num_of_versions * db_num + version) / (num_of_dbs * num_of_versions)) * 100) + '%')\n",
    "            \n",
    "node_data = pd.DataFrame(data)\n",
    "node_data.set_index(\"db_name\", inplace=True)\n",
    "node_data.index.name = None\n",
    "node_data.fillna(0, inplace=True)\n",
    "for column in node_data.columns:\n",
    "    node_data[column] = node_data[column] /node_data[column].abs().max()\n",
    "node_data.fillna(0, inplace=True)\n",
    "node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.DataFrame(mapped_edges, columns=[\"source\", \"target\"])\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca12081",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg = StellarGraph({\"db\": node_data}, {\"next\": edges})\n",
    "print(sdg.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "results = pd.DataFrame([])\n",
    "result_a = []\n",
    "for layer_size in range (1, 3):\n",
    "    for train_size in range(1, 10):\n",
    "        for test_size in range(1, 10):\n",
    "            train_acc, train_loss, test_acc, test_loss = train_gcn(sdg, train_size * 0.1, test_size * 0.1, epochs, layer_size * 16)\n",
    "            result_a.append({\n",
    "                'config': 'Layer Size: ' + str(layer_size * 16) + ' Train Size: ' + str(round(train_size * 0.1, 1)) + ' Test Size: ' + str(round(test_size * 0.1, 1)),\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "                'layer_size': layer_size,\n",
    "                'train_size': train_size,\n",
    "                'test_size': test_size\n",
    "            })\n",
    "            print('Progress: ' + str(len(result_a)/2) + '%')\n",
    "results = pd.DataFrame(result_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "results.sort_values(by=['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ten_results = pd.DataFrame(results.sort_values(by=['test_acc']).iloc[-10:])\n",
    "best_ten_results\n",
    "avg_results_a = []\n",
    "for index, row in best_ten_results.iterrows():\n",
    "    train_acc_a = []\n",
    "    train_loss_a = []\n",
    "    test_acc_a = []\n",
    "    test_loss_a = []\n",
    "    for i in range(0, 10):\n",
    "        train_acc, train_loss, test_acc, test_loss = train_gcn(sdg, row['train_size'] * 0.1, row['test_size'] * 0.1, 50, row['layer_size'] * 16)\n",
    "        train_acc_a.append(train_acc)\n",
    "        train_loss_a.append(train_loss)\n",
    "        test_acc_a.append(test_acc)\n",
    "        test_loss_a.append(test_loss)\n",
    "    print(train_acc_a)\n",
    "    avg_results_a.append({\n",
    "        'config': 'Layer Size: ' + str(row['layer_size'] * 16) + ' Train Size: ' + str(round(row['train_size'] * 0.1, 1)) + ' Test Size: ' + str(round(row['test_size'] * 0.1, 1)),\n",
    "        'Layer Size': row['layer_size'] * 16,\n",
    "        'Train Size': round(row['train_size'] * 0.1, 1),\n",
    "        'Test Size': round(row['test_size'] * 0.1, 1),\n",
    "        'train_acc': norm.fit(train_acc_a)[0],\n",
    "        'train_loss': norm.fit(train_loss_a)[0],\n",
    "        'test_acc': norm.fit(test_acc_a)[0],\n",
    "        'test_loss': norm.fit(test_loss_a)[0]       \n",
    "    })\n",
    "avg_results = pd.DataFrame(avg_results_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results.to_excel(\"output-sqlite.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
